# Lilo Take Home Test - Approaches and Solutions

This document details three distinct approaches developed to categorize products into their correct categories. Each method was designed to address shortcomings of previous iterations and enhance overall accuracy, robustness, and scalability. The system uses a Chroma vector database ("cromha") to store vector representations, and the product text is generated by uniting key columns (e.g., metaTitle and vendorCategory) from the CSV.

## Table of Contents
- [Approach 1: Single Embedding Model with Direct Similarity Search](#approach-1-single-embedding-model-with-direct-similarity-search)
- [Approach 2: Voting-based Similarity Search](#approach-2-voting-based-similarity-search)
- [Approach 3: Multi-embedding Model Ensemble with Parallelization](#approach-3-multi-embedding-model-ensemble-with-parallelization)
- [Comparison of Approaches](#comparison-of-approaches)
- [Conclusion](#conclusion)
- [Future Work](#future-work)
- [Installation and Execution Instructions](#installation-and-execution-instructions)

---

## Approach 1: Single Embedding Model with Direct Similarity Search

In this approach, a single embedding model is used to convert both category and product texts into vector representations. The process is as follows:

- **Preprocessing and Data Preparation:**  
  The `categories.csv` file is read and cleaned. Each category is processed to obtain both the full path (e.g., `facilities/gym/machines`) and its final component (e.g., `machines`). For products (from `products.csv`), key columns (like `metaTitle` and `vendorCategory`) are merged into a unified text representation that forms the basis for generating embeddings.

- **Vectorization and Storage:**  
  Both categories and products are embedded using the selected model (default is the local **deepseek-r1:8b** model, though OpenAI embeddings can be used optionally). The resulting embeddings are stored in a Chroma vector database.

- **Similarity Search:**  
  Each product’s embedding is compared against the category embeddings using a direct similarity search. The category with the highest similarity score is assigned to the product.

This method is simple and fast, but its reliance on a single search strategy can make it vulnerable to noise or ambiguous product descriptions.

---

## Approach 2: Voting-based Similarity Search

The second approach improves on the first by incorporating multiple similarity search techniques and a voting mechanism:

- **Multiple Similarity Searches:**  
  For every product, several search methods are applied:
  - **Standard Similarity Search:** Performed on both the full category text ("ALL") and its last component ("LAST").
  - **Max Marginal Relevance Search:** Diversifies the candidate selection to avoid redundancy.

- **Voting Mechanism:**  
  Each search method returns a list of candidate categories with weighted relevance scores. These votes are aggregated so that the category with the highest total vote is chosen. This voting helps to smooth out errors from any single method.

This approach adds robustness by reducing the likelihood of misclassification through a consensus-based decision, at the expense of increased computational overhead.

---

## Approach 3: Multi-embedding Model Ensemble with Parallelization

The third and most advanced approach builds on the previous solutions by combining multiple word embedding models and leveraging parallel processing:

- **Ensemble of Embedding Models:**  
  An array of embedding models (e.g., `"bge-m3"`, `"granite-embedding:278m"`, `"snowflake-arctic-embed2"`, `"avr/sfr-embedding-mistral"`, `"mxbai-embed-large"`, and `"nomic-embed-text"`) is used. Each model generates its own vector representations, and dedicated Chroma collections are maintained for every model.

- **Parallel Execution:**  
  To handle large datasets efficiently, the product list is divided among multiple processes using a parallelization script (`run_parallel.py`). Each process executes a part of the multi-embedding voting strategy (implemented in `RunOneLLM_MultEmbVoteDIVIDE.py`).

- **Aggregated Voting Across Models:**  
  Similarity searches are performed across each model’s Chroma collection. Votes from all models are aggregated to select the final category, ensuring that the diverse perspectives of the models improve reliability.

This approach is the most robust and scalable, though it requires greater computational resources and a more complex deployment setup.

---

## Comparison of Approaches

| Aspect            | Approach 1                                     | Approach 2                                               | Approach 3                                                    |
|-------------------|------------------------------------------------|----------------------------------------------------------|---------------------------------------------------------------|
| **Complexity**    | Low – simple single model implementation       | Medium – multiple search techniques with voting           | High – ensemble of models with parallel processing             |
| **Robustness**    | Vulnerable to noise or ambiguity               | Improved through vote aggregation                        | Highest, by leveraging diverse model outputs and ensemble voting |
| **Performance**   | Fast execution but limited by single-method accuracy | Slightly slower due to multiple searches                  | More resource-intensive, mitigated via parallelization           |
| **Scalability**   | Limited scalability                            | Better scalability with sequential execution              | Excellent scalability with workload division across processes     |

---

## Conclusion

The three approaches illustrate an evolutionary development process:
- **Approach 1** establishes the basic workflow using a single embedding model and direct similarity search.
- **Approach 2** enhances reliability through multiple search strategies and a weighted voting mechanism.
- **Approach 3** maximizes accuracy and scalability by employing an ensemble of embedding models and parallelizing the workload.

Each method was developed to progressively overcome previous limitations, ensuring that the system is both robust and adaptable to varying data volumes and complexities.

---

## Future Work

Future enhancements will focus on integrating an advanced voting process mediated by an agent LLM. This agent would:
- **Verify Coherence:** Analyze the coherence of candidate category selections generated by the ensemble voting process.
- **Select the Best Response:** Utilize its reasoning capabilities to select the most consistent and accurate category from the aggregated results.
- **Refine the Voting Mechanism:** Enhance the current weighted voting process by incorporating contextual analysis and cross-model consistency checks.

Additionally, further exploration will be done on optimizing the union of columns (e.g., merging metaTitle, vendorCategory, and other relevant fields) to generate richer text representations for more effective embeddings. Improvements to the Chroma vector store usage (referred to as "cromha" in the code) will also be considered, aiming to streamline document addition and retrieval processes.

---

## Installation and Execution Instructions

1. **Create and Activate a Virtual Environment:**
   ```bash
   python -m venv /LILOTest
   c:\LILOTest\Scripts\activate
   ```

2. **Upgrade pip and Install Dependencies:**

```bash

pip install --upgrade pip
pip install -r requeriments.txt
```
2.1 **Alternatively, install dependencies step-by-step:**

```bash
pip install -U pip setuptools wheel
pip install -U spacy
pip install seaborn
pip install scikit-learn
pip install pandas
pip install gensim
pip install langchain chromadb gradio ollama pypdf
pip install openai
pip install -U langchain-community
pip install langchain_ollama
pip install langchain_openai
pip install langchain-chroma
pip install nltk
```
3. **Install and Set Up Ollama:**

For Linux:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```
For Windows/Mac:
Download and install from: ollama.com
Download the Required Models:
Main Embedding Model (default: deepseek-r1:8b):
```bash
ollama pull deepseek-r1:8b
```
Additional Word Embedding Models (used in Approach 3):
```bash
ollama pull bge-m3
ollama pull granite-embedding:278m
ollama pull snowflake-arctic-embed2
ollama pull avr/sfr-embedding-mistral
ollama pull mxbai-embed-large
ollama pull nomic-embed-text
```
4. **Running the Solutions:**
Change directory to the ...LILO\LILO-Categories-Test\LocalLLM
* Approach 1:
Run `python RunOneLLMEmb.py` to execute the direct similarity search approach.
* Approach 2:
Run `python RunOneLLMEmbVote.py` to execute the voting-based similarity search.
* Approach 3:
Execute `python run_parallel.py` to start the parallel processing of the multi-embedding ensemble approach. This script calls `RunOneLLM_MultEmbVoteDIVIDE.py` internally for each process.
For more details on the commands and environment setup, please refer to the accompanying UTILS.md. (or `python RunOneLLM_MultEmbVote.py` for secuential)
